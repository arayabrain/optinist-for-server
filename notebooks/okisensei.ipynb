{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy import sparse\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import convolve\n",
    "from skimage import measure\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize random number generator\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for input data (Caiman, Suite2p, synthetic)\n",
    "\n",
    "def cnmf_data_to_standard(A_or, timecourse):\n",
    "    \"\"\"Convert CaImAn CNMF data to standardized format.\"\"\"\n",
    "    if sparse.issparse(A_or):\n",
    "        A_or = A_or.toarray()\n",
    "    \n",
    "    n_pixels = A_or.shape[0]\n",
    "    fov_size = int(np.sqrt(n_pixels))  # Assuming square FOV\n",
    "    n_cells = A_or.shape[1]\n",
    "    \n",
    "    A_or_full = A_or.reshape((fov_size, fov_size, n_cells), order='F')\n",
    "    labelimage = np.zeros((fov_size, fov_size))\n",
    "    \n",
    "    for n in range(n_cells):\n",
    "        mask = A_or_full[..., n] > 0\n",
    "        labelimage[mask] = n + 1\n",
    "        \n",
    "    # Handle ROI overlap\n",
    "    unique_labels = np.unique(labelimage[labelimage > 0])\n",
    "    missing_cells = set(range(1, n_cells + 1)) - set(unique_labels)\n",
    "    \n",
    "    for c in missing_cells:\n",
    "        cell_mask = A_or_full[..., c-1] > 0\n",
    "        props = measure.regionprops(cell_mask.astype(int))\n",
    "        if props:\n",
    "            y, x = map(int, props[0].centroid)\n",
    "            labelimage[y, x] = c\n",
    "    \n",
    "    return {'timecourse': timecourse, 'labelimage': labelimage}\n",
    "\n",
    "def generate_synthetic_data(n_neurons=100, n_timepoints=3000, n_clusters=5, fov_size=512):\n",
    "    \"\"\"Generate synthetic data in standardized format.\"\"\"\n",
    "    neurons_per_cluster = n_neurons // n_clusters\n",
    "    labelimage = np.zeros((fov_size, fov_size))\n",
    "    \n",
    "    # Generate spatial layout\n",
    "    cluster_centers = np.array([\n",
    "        [0.2, 0.2],  # Top left\n",
    "        [0.8, 0.2],  # Top right\n",
    "        [0.5, 0.5],  # Center\n",
    "        [0.2, 0.8],  # Bottom left\n",
    "        [0.8, 0.8]   # Bottom right\n",
    "    ]) * fov_size\n",
    "    \n",
    "    cell_radius = int(fov_size * 0.015)  # Scale with FOV\n",
    "    cell_id = 1\n",
    "    \n",
    "    for cluster in range(n_clusters):\n",
    "        center = cluster_centers[cluster]\n",
    "        radius = fov_size * 0.15\n",
    "        theta = 2 * np.pi * np.random.rand(neurons_per_cluster)\n",
    "        r = radius * np.sqrt(np.random.rand(neurons_per_cluster))\n",
    "        \n",
    "        x = center[0] + r * np.cos(theta)\n",
    "        y = center[1] + r * np.sin(theta)\n",
    "        \n",
    "        for i in range(neurons_per_cluster):\n",
    "            if all(coord > cell_radius and coord < fov_size - cell_radius \n",
    "                  for coord in [x[i], y[i]]):\n",
    "                XX, YY = np.meshgrid(np.arange(fov_size), np.arange(fov_size))\n",
    "                mask = ((XX - x[i])**2 + (YY - y[i])**2 <= cell_radius**2)\n",
    "                if not np.any(labelimage[mask]):\n",
    "                    labelimage[mask] = cell_id\n",
    "                    cell_id += 1\n",
    "    \n",
    "    # Generate temporal patterns\n",
    "    t = np.arange(n_timepoints)\n",
    "    timecourse = np.zeros((n_neurons, n_timepoints))\n",
    "    frequencies = np.linspace(0.002, 0.006, n_clusters)\n",
    "    amplitudes = np.linspace(1.5, 2.5, n_clusters)\n",
    "    \n",
    "    for i in range(n_clusters):\n",
    "        period = n_timepoints // n_clusters\n",
    "        start = i * period\n",
    "        end = start + period\n",
    "        \n",
    "        pattern = amplitudes[i] * np.sin(2 * np.pi * frequencies[i] * (t - start))\n",
    "        onset = 1 / (1 + np.exp(-(t - start) / 100))\n",
    "        offset = 1 / (1 + np.exp((t - end) / 100))\n",
    "        pattern = pattern * onset * offset\n",
    "        \n",
    "        # Add cluster variations\n",
    "        if i % 2 == 0:\n",
    "            pattern += 0.5 * np.sin(2 * np.pi * 0.05 * (t - start))\n",
    "        else:\n",
    "            ramp = np.clip((t - start) / period, 0, 1)\n",
    "            pattern *= (1 + ramp * 0.5)\n",
    "            \n",
    "        pattern = convolve(pattern, np.exp(-np.arange(50)/5), mode='same')\n",
    "        \n",
    "        cluster_neurons = slice(i * neurons_per_cluster, (i + 1) * neurons_per_cluster)\n",
    "        for j, idx in enumerate(range(*cluster_neurons.indices(n_neurons))):\n",
    "            timecourse[idx] = (pattern + \n",
    "                             0.1 * np.sin(2 * np.pi * 0.001 * t + j) + \n",
    "                             0.05 * np.random.randn(n_timepoints))\n",
    "    \n",
    "    return {'timecourse': timecourse, 'labelimage': labelimage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataType = 'synthetic'  # 'synthetic', 'suite2p', 'caiman'\n",
    "if DataType == 'synthetic':\n",
    "    # Can specify FOV size if needed\n",
    "    data = generate_synthetic_data(n_neurons=100, n_timepoints=3000, n_clusters=5, fov_size=512)\n",
    "else: \n",
    "    input_data = loadmat('M000025_ori018_timecourse.mat')\n",
    "    if DataType == 'caiman':\n",
    "        data = cnmf_data_to_standard(input_data['A_or'], input_data['timecourse'])\n",
    "    # else:\n",
    "    #     data = suite2p_data_to_standard(input_data['stat'], input_data['timecourse'])\n",
    "\n",
    "timecourse = data['timecourse']\n",
    "labelimage = data['labelimage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1: Cell locations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(labelimage, cmap='nipy_spectral')\n",
    "plt.title('Cell Locations')\n",
    "plt.colorbar(label='Cell ID')\n",
    "\n",
    "# Subplot 2: Example time courses for each cluster\n",
    "plt.subplot(1, 2, 2)\n",
    "t = np.arange(timecourse.shape[1])\n",
    "for i in range(timecourse.shape[0]):\n",
    "    plt.plot(t, timecourse[i, :], label=f'Neuron {i + 1}')\n",
    "plt.title('Example Time Courses for Each Cluster')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Activity')\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.legend(handles[:10], labels[:10])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calculate correlation coefficients\n",
    "# Compute pairwise correlations between all cells\n",
    "corr_matrix = np.corrcoef(timecourse)\n",
    "\n",
    "# Determine optimal number of clusters using both Elbow method and Silhouette score\n",
    "k_range = range(2, 21)  # Test cluster numbers from 2 to 20\n",
    "# elbow_values = []\n",
    "silhouette_values = []\n",
    "\n",
    "# Iterate through different numbers of clusters to find optimal k\n",
    "for k in k_range:\n",
    "    # Perform k-means clustering with 10 different random initializations\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=10)\n",
    "    labels = kmeans.fit_predict(corr_matrix)\n",
    "    \n",
    "    # Calculate metrics for determining optimal k\n",
    "    # elbow_values.append(kmeans.inertia_)  # Sum of within-cluster distances for Elbow method\n",
    "    silhouette_values.append(silhouette_score(corr_matrix, labels))  # Average silhouette score\n",
    "\n",
    "# 2) Perform k-means with optimal k and 10 different random initializations\n",
    "# Choose k with highest silhouette score\n",
    "best_k_idx = np.argmax(silhouette_values)\n",
    "k_optimal = k_range[best_k_idx]\n",
    "kmeans = KMeans(n_clusters=k_optimal, init='k-means++', n_init=10, random_state=42)\n",
    "idx_kmeans = kmeans.fit_predict(corr_matrix)\n",
    "\n",
    "# Store all clustering results in a dictionary\n",
    "clustering_results = {\n",
    "    'kmeans': idx_kmeans,\n",
    "}\n",
    "\n",
    "# 3) Reorder correlation matrix based on cluster assignments\n",
    "# Reorder for k-means results\n",
    "sort_idx_kmeans = np.argsort(idx_kmeans)\n",
    "sorted_corr_matrix_kmeans = corr_matrix[sort_idx_kmeans][:, sort_idx_kmeans]\n",
    "\n",
    "# 4) Mean time course for each cluster\n",
    "unique_clusters = np.unique(idx_kmeans)\n",
    "n_clusters = len(unique_clusters)\n",
    "colors = plt.cm.jet(np.linspace(0, 1, n_clusters))\n",
    "custom_cmap = ListedColormap(colors)\n",
    "\n",
    "cluster_averages = []\n",
    "for cluster in unique_clusters:\n",
    "    cluster_mask = idx_kmeans == cluster  # This creates boolean mask of shape (n_cells,)\n",
    "    cluster_avg = np.mean(timecourse[cluster_mask], axis=0)  # Average of selected cells\n",
    "    cluster_averages.append(cluster_avg)\n",
    "\n",
    "# 5) Cluster maps\n",
    "cluster_map = np.zeros_like(labelimage)\n",
    "for cell_idx in range(1, int(np.max(labelimage)) + 1):\n",
    "    if cell_idx <= len(idx_kmeans):\n",
    "        roi_mask = labelimage == cell_idx\n",
    "        cluster_map[roi_mask] = idx_kmeans[cell_idx - 1] + 1  # +1 so clusters start at 1 not 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization should be:\n",
    "plt.figure(figsize=(15, 5))\n",
    "# 1. Correlation matrix\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(sorted_corr_matrix_kmeans, cmap='jet')\n",
    "plt.colorbar()\n",
    "plt.title(f'k-means Clustering (k = {k_optimal})')\n",
    "plt.xlabel('Cells')\n",
    "plt.ylabel('Cells')\n",
    "\n",
    "save_thumbnails()\n",
    "\n",
    "# 2. Average time courses\n",
    "plt.subplot(1, 3, 2)\n",
    "for i, avg in enumerate(cluster_averages):\n",
    "    plt.plot(avg, color=colors[i], label=f'Cluster {i+1}')\n",
    "plt.title(f'Mean time course of each cluster')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Cluster average')\n",
    "plt.legend()\n",
    "\n",
    "# 3. Cell maps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(cluster_map, cmap=custom_cmap)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('Cluster assignments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "score = pca.fit_transform(timecourse.T)  # Transpose to match MATLAB's orientation\n",
    "coeff = pca.components_.T  # Get eigenvectors (components)\n",
    "explained = pca.explained_variance_ratio_ * 100  # Convert to percentage\n",
    "\n",
    "# Components to visualize\n",
    "num_components = min(timecourse.shape[0], 50)\n",
    "num_components = 2  # Use 2 components for development\n",
    "\n",
    "# Setup visualization\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot spatial maps and temporal components side by side\n",
    "for i in range(num_components):\n",
    "    # Calculate contribution map\n",
    "    contribution = np.abs(coeff[:, i])\n",
    "    contribution_map = np.zeros_like(labelimage, dtype=float)\n",
    "    \n",
    "    # Fill in contribution map\n",
    "    for cell_idx in range(1, int(np.max(labelimage)) + 1):\n",
    "        contribution_map[labelimage == cell_idx] = contribution[cell_idx - 1]\n",
    "    \n",
    "    # Create subplot for spatial map\n",
    "    ax = plt.subplot(num_components, 2, 2 * i + 1)  # Odd indices: 1, 3, 5, ...\n",
    "    im = ax.imshow(contribution_map, cmap='jet')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_title(f'PC {i+1} Spatial Map')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create subplot for temporal component\n",
    "    ax = plt.subplot(num_components, 2, 2 * i + 2)  # Even indices: 2, 4, 6, ...\n",
    "    ax.plot(score[:, i])\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel(f'PC {i+1}')\n",
    "    ax.set_title(f'PC {i+1} Temporal Profile')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure with subplots\n",
    "output_filename = \"pca_2by2_subplots.png\"  # You can change the file format (e.g., .pdf, .svg)\n",
    "plt.savefig(output_filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Figure saved as {output_filename}\")\n",
    "\n",
    "# Create new figure for explained variance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(explained[:10], '-o')\n",
    "plt.xlabel('PC Component')\n",
    "plt.ylabel('Variance Explained (%)')\n",
    "plt.title('Explained Variance')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save the explained variance plot\n",
    "variance_filename = \"explained_variance.png\"\n",
    "plt.savefig(variance_filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Explained variance plot saved as {variance_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Save thumbnail function\n",
    "def save_thumbnail(plot_file, thumbnail_height=300):\n",
    "    with Image.open(plot_file) as img:\n",
    "        w, h = img.size\n",
    "        new_width = int(w * (thumbnail_height / h))\n",
    "        thumb_img = img.resize((new_width, thumbnail_height), Image.Resampling.LANCZOS)\n",
    "        thumb_img.save(plot_file.replace(\".png\", \".thumb.png\"))\n",
    "\n",
    "# Updated PCA visualization function\n",
    "def generate_pca_visualization(scores, explained_variance, components, roi_masks, output_dir):\n",
    "    \"\"\"\n",
    "    Generate standalone PCA visualizations for each component\n",
    "    \"\"\"\n",
    "    num_components = min(3, components.shape[0], scores.shape[1])\n",
    "    \n",
    "    # Plot explained variance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    num_display = min(10, len(explained_variance))\n",
    "    bars = plt.bar(range(1, num_display + 1), explained_variance[:num_display])\n",
    "    plt.title(\"Explained Variance\")\n",
    "    plt.xlabel(\"Principal Component\")\n",
    "    plt.ylabel(\"Explained Variance (%)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    explained_var_path = os.path.join(output_dir, \"pca_explained_variance.png\")\n",
    "    plt.savefig(explained_var_path)\n",
    "    plt.close()\n",
    "    save_thumbnail(explained_var_path)\n",
    "    print(f\"Explained variance plot saved to {explained_var_path}\")\n",
    "\n",
    "    # Plot each component separately\n",
    "    for i in range(num_components):\n",
    "        # Time course plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(scores[:, i], linewidth=2)\n",
    "        plt.title(f\"PC {i+1} Time Course\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Component Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        time_course_path = os.path.join(output_dir, f\"pca_pc{i+1}_time_course.png\")\n",
    "        plt.savefig(time_course_path)\n",
    "        plt.close()\n",
    "        save_thumbnail(time_course_path)\n",
    "        print(f\"PC {i+1} time course plot saved to {time_course_path}\")\n",
    "\n",
    "        # Spatial map plot\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        component_weights = np.abs(components[i])\n",
    "        component_map = np.zeros(roi_masks.shape[:2])\n",
    "        for cell_idx in range(roi_masks.shape[2]):\n",
    "            if cell_idx < len(component_weights):\n",
    "                weight = component_weights[cell_idx]\n",
    "                roi_mask = roi_masks[..., cell_idx]\n",
    "                component_map[roi_mask > 0] = weight\n",
    "        \n",
    "        plt.imshow(component_map, cmap=\"viridis\")\n",
    "        plt.colorbar(label=\"Component Weight\")\n",
    "        plt.title(f\"PC {i+1} Spatial Map\")\n",
    "        plt.tight_layout()\n",
    "        spatial_map_path = os.path.join(output_dir, f\"pca_pc{i+1}_spatial_map.png\")\n",
    "        plt.savefig(spatial_map_path)\n",
    "        plt.close()\n",
    "        save_thumbnail(spatial_map_path)\n",
    "        print(f\"PC {i+1} spatial map plot saved to {spatial_map_path}\")\n",
    "\n",
    "# Updated K-means visualization function\n",
    "def generate_kmeans_visualization(labels, corr_matrix, fluorescence, roi_masks, output_dir):\n",
    "    \"\"\"\n",
    "    Generate standalone K-means visualizations\n",
    "    \"\"\"\n",
    "    # Reorder correlation matrix based on clusters\n",
    "    sort_idx = np.argsort(labels)\n",
    "    sorted_corr_matrix = corr_matrix[sort_idx][:, sort_idx]\n",
    "    \n",
    "    # Calculate cluster information\n",
    "    unique_clusters = np.unique(labels)\n",
    "    n_clusters = len(unique_clusters)\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n_clusters)) \n",
    "    custom_cmap = ListedColormap(colors)\n",
    "    \n",
    "    # Calculate mean time course for each cluster\n",
    "    cluster_averages = []\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_mask = labels == cluster\n",
    "        cluster_avg = np.mean(fluorescence[cluster_mask], axis=0)\n",
    "        cluster_averages.append(cluster_avg)\n",
    "    \n",
    "    # Generate cluster map\n",
    "    cluster_map = np.zeros(roi_masks.shape[:2])\n",
    "    for i, label in enumerate(labels):\n",
    "        if i < roi_masks.shape[2]:\n",
    "            roi_mask = roi_masks[..., i]\n",
    "            cluster_map[roi_mask > 0] = label + 1\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.imshow(sorted_corr_matrix, cmap=\"jet\")\n",
    "    plt.colorbar(label=\"Correlation\")\n",
    "    plt.title(f\"K-means Clustering (k={n_clusters})\")\n",
    "    plt.xlabel(\"Cells\")\n",
    "    plt.ylabel(\"Cells\")\n",
    "    plt.tight_layout()\n",
    "    corr_matrix_path = os.path.join(output_dir, \"kmeans_correlation_matrix.png\")\n",
    "    plt.savefig(corr_matrix_path)\n",
    "    plt.close()\n",
    "    save_thumbnail(corr_matrix_path)\n",
    "    print(f\"Correlation matrix plot saved to {corr_matrix_path}\")\n",
    "\n",
    "    # Plot average time courses\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for i, avg in enumerate(cluster_averages):\n",
    "        plt.plot(avg, color=colors[i], linewidth=2, label=f\"Cluster {i+1}\")\n",
    "    plt.title(\"Mean Time Course by Cluster\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fluorescence\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    time_course_path = os.path.join(output_dir, \"kmeans_time_courses.png\")\n",
    "    plt.savefig(time_course_path)\n",
    "    plt.close()\n",
    "    save_thumbnail(time_course_path)\n",
    "    print(f\"Time course plot saved to {time_course_path}\")\n",
    "\n",
    "    # Plot cluster map\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.imshow(cluster_map, cmap=custom_cmap)\n",
    "    plt.colorbar(label=\"Cluster\")\n",
    "    plt.title(\"Cluster Assignments\")\n",
    "    plt.tight_layout()\n",
    "    cluster_map_path = os.path.join(output_dir, \"kmeans_cluster_map.png\")\n",
    "    plt.savefig(cluster_map_path)\n",
    "    plt.close()\n",
    "    save_thumbnail(cluster_map_path)\n",
    "    print(f\"Cluster map plot saved to {cluster_map_path}\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"./test_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Test PCA visualization\n",
    "pca_scores = score  # Replace with actual data\n",
    "pca_explained_variance = explained  # Replace with actual data\n",
    "pca_components = coeff.T  # Replace with actual data\n",
    "roi_masks = np.zeros((labelimage.shape[0], labelimage.shape[1], int(np.max(labelimage))))\n",
    "for cell_idx in range(1, int(np.max(labelimage)) + 1):\n",
    "    roi_masks[:, :, cell_idx-1] = (labelimage == cell_idx)\n",
    "\n",
    "generate_pca_visualization(pca_scores, pca_explained_variance, pca_components, roi_masks, output_dir)\n",
    "\n",
    "# Test K-means visualization\n",
    "kmeans_labels = idx_kmeans  # Replace with actual data\n",
    "kmeans_corr_matrix = corr_matrix  # Replace with actual data\n",
    "kmeans_fluorescence = timecourse  # Replace with actual data\n",
    "\n",
    "generate_kmeans_visualization(kmeans_labels, kmeans_corr_matrix, kmeans_fluorescence, roi_masks, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPLABCUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
